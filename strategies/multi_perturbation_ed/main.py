import numpy as np
import itertools
import scipy.optimize
import networkx as nx
import matplotlib.pyplot as plt
import math
from matplotlib.lines import Line2D
import time
import strategies.multi_perturbation_ed.params
import json
import sys
import strategies.multi_perturbation_ed.mec_size
import networkx.algorithms.approximation.vertex_cover as vertex_cover
import strategies.multi_perturbation_ed.dream

"""
Represented by a binary matrix where m[i, j]= 1 iff i->j, else 0

PCDAG objects:
Represented by an integer matrix where m[i, j]= 1 iff i->j, -1 iff i-j (and edge no known), else 0

Weight matrices:
For linear models we store the weights as a matrix of floats. For infinite samples we don't need a weight matrix.

Noise:
Noise scales in gaussian linear models are stored as a vectors of floats
"""

def generate_chain_dag_fixed_root(n, v):
    """
    generates a DAG that is just a chain with no colliders, but may not just be directions
    left to right (the root vertex is random).
    input:
    int n: number of nodes
    output:
    matrix: chain dag with no unshielded colliders
    """
    dag_temp = np.zeros((n,n))

    for i in range(0, v):
        dag_temp[i+1, i] = 1

    for i in range(v, n-1):
        dag_temp[i, i+1] = 1

    return dag_temp

def generate_barabasi_albert(n, m1, m2=1, p=1):
    #Generates a scale-free dag using the Barabasi-Albert Model
    #each node is attached m1 times with prob p, else m2 times
    #0 will always be the root
    g = nx.dual_barabasi_albert_graph(n, m1, m2, p)
    dag = np.triu(nx.to_numpy_matrix(g), k=1)
    np.random.shuffle(dag)
    return dag

def uniform_random_tree(n):
    """
    generates tree MEC uniformly, then picks a node to be the root
    https://nokyotsu.com/qscripts/2008/05/generating-random-trees-and-connected.html
    input:
    int n: tree size
    output:
    matrix: dag with tree mec
    """
    cpdag = np.zeros((n, n))

    src = []
    dst = np.random.permutation(np.arange(n)).tolist()
    src.append(dst.pop()) # picks the root

    while(len(dst)>0):
        a = np.random.choice(src) #random element in src
        b = dst.pop()
        # add edge a,b
        cpdag[a][b] = -1
        cpdag[b][a] = -1
        src.append(b)

    #now randomly pick root and orient
    dag_temp = orient_tree_root_v(cpdag, np.random.choice(n))

    return dag_temp

def generate_k_star_system(n, k):
    """
    generates a forest of k stars with n nodes
    input:
    int n: tree size
    int k: number of stars
    output:
    matrix: dag with tree mec
    """
    dag = np.zeros((n,n))
    r = int(math.ceil(n/k))
    host = 0
    nodes = np.arange(n)
    np.random.shuffle(nodes)

    for i in range(n):
        if i%r == 0:
            host = i
        else:
            #start undirected
            dag[nodes[host], nodes[i]] = 1

    return dag

def generate_fully_connected(n):
    """
    generate a fully connected dag with a fully connected mec (no colliders)
    input:
    int n, number of nodes
    output:
    matrix dag
    """
    #take matrix of all ones, removes the diagonal and below
    #only generates DAGs where the ordering is the ordering of the nodes
    #but this is fine since all methods are agnostic to this symmetry
    dag = np.triu(np.ones(n),k=1)
    return dag

def ER_bipartite(n, m, p):
    """
    constructs a bipartite graph under an ER-like model
    input:
    int n: the number of nodes in set one
    int m: the number of nodes in set 2
    float p: the edge probability param
    """
    #strategy: generate undirected graph and remove all below diagonal
    nx_dag = nx.bipartite.random_graph(n, m, p, directed=False)

    return np.triu(nx.to_numpy_array(nx_dag), 1)

def generate_ER(n, p):
    """
    generates a dag using an Erdos renyi model
    first generates a random ordering, then
    input:
    int n, number of nodes
    float p, probability of each edge appearing
    output:
    matrix dag
    """
    ordering = np.arange(n)
    np.random.shuffle(ordering)
    #each element ordering[i] is node indexd by i's place in the ordering

    dag = np.zeros((n,n))

    for i in range(n):
        for j in range(i, n):
            if i == j:
                continue
            if np.random.binomial(1, p) == 1:
                if ordering[i] > ordering[j]:
                    dag[j,i] = 1
                else:
                    dag[i,j] = 1

    return dag

def generate_chain_dag(n):
    """
    generates a chain DAG that may have colliders. generated by permuting variables
    input:
    int n: number of nodes
    output:
    matrix: chain dag potentially with shielded colliders
    """
    #the nodes go in order but we just sample randomly whether the arrow goes forwards or backwards
    forward_backwards = np.random.binomial(1, 0.5, n)
    dag = np.zeros((n,n),dtype=np.int32)
    for i in range(0, n-1): #don't let the last noe connect back to the first or get a cycle
        if forward_backwards[i] == 0:
            dag[i][i+1] = 1
        else:
            dag[i+1][i] = 1
    return dag


def extract_undirected(cpdag, v):
    """
    given a cpdag and a node v, get all undirected edges out of v
    input:
    matrix cpdag
    node v
    output:
    array containing all edges with which vhas an undirected edge with
    """
    return np.flatnonzero(np.minimum(cpdag[v], 0))

def extract_all_directed(cpdag):
    """
    given a cpdag , extract all edges that are directed
    input:
    matrix cpdag
    output:
    list of tuples of form (u, v) where u->v
    """
    n = cpdag.shape[0]
    edges = []
    for v in range(0, n):
        for u in range(0, n):
            if cpdag[v][u] == 1:
                edges.append((v, u))
    return edges


def chordal_random_intervention(cpdag, k):
    """
    generate a chordal random intervention of size up to k
    by chordal random we just mean intervene on nodes adjacent to undirected edges
    input:
    matrix cpdag: the current cpdag
    int k: the number of perturbations
    output:
    list: the nodes we will perturb
    """

    #first extract all nodes with undirected edges next to them
    #do this by taking the min of each row (if there is a -1 we will get -1)
    #then select all the indices with a -1
    min_val = np.amin(cpdag, axis=1)
    possible_nodes = np.flatnonzero(np.minimum(min_val, 0)) #the min makes everthing -1 or 0

    #second randomly sample from the nodes we just extracted
    #even if you don't fill up the constraints (due to too much being identified
    #already),this is ok
    return np.random.choice(possible_nodes, size = min(k, np.size(possible_nodes)), replace = False)

def chordal_random_intervention_set(cpdag, n_b, k):
    """
    generates a batch of chordal random interventions
    input:
    matrix cpdag: current cpdag
    int n_b: interventions in the batch
    int k: max number of perturbations per intervention

    output:
    list of lists of ints intervention_set
    """
    intervention_set = []
    for _ in range(n_b):
        intervention = chordal_random_intervention(cpdag, k)
        intervention_set.append(intervention)
    return intervention_set

def meek(cpdag, new_edges=None, skip_r3=False, is_tree=False):
    """
    applies rules R1 to R4
    input:
    matrix cpdag
    list new_edges, the list of new edges that we start with for meek rules. None means use all directed edges
    bool skip_r3, if True does not consider r3. This can be used when working with interventions, since R3 is only
    used with colliders, and interventions don't uncover new colliders
    bool is_tree: if the mec inputted is a tree, means we need only do R1
    output:
    matrx cpdag
    """
    #keep going til there is a round where you find no edge
    edges_found = True
    latest_edges = new_edges
    if new_edges == None:
        latest_edges = extract_all_directed(cpdag)
    while edges_found:
        edges_found = False
        latest_edges_temp = []
        #for R1 just need to look at recently oriented edges (u, v) and orient all undirected
        #(v, d) where there is not edge between d and u
        #print("R1 time")
        start_time = time.time()
        for edge in latest_edges:
            v = edge[1]
            u = edge[0]
            possible_orients = extract_undirected(cpdag, v)
            for d in possible_orients:
                #only orient if not yet oriented
                if cpdag[v][d] == -1:
                    #no edge to complete the triangle
                    if cpdag[d][u] == 0 and cpdag[u][d] == 0 and u!=d:
                        cpdag[v][d] = 1
                        cpdag[d][v] = 0

                        edges_found = True
                        if((v,d)) not in latest_edges_temp:
                            latest_edges_temp.append((v, d))
        #for R2 look at recently oriented edges (u, v), check if any in a cycle pattern,
        #orient
        if not is_tree:
            start_time = time.time()
            for edge in latest_edges:
                v = edge[1]
                u = edge[0]
                #now check all edges going into u
                nodes_into = np.flatnonzero(np.maximum(cpdag[:, u], 0))
                for w in nodes_into:
                    #if there exists w-v, orient it as w->v
                    if cpdag[w, v] == -1:
                        cpdag[w, v] = 1
                        cpdag[v, w] = 0

                        edges_found = True
                        if((w,v)) not in latest_edges_temp:
                            latest_edges_temp.append((w, v))

                #now check all edges going out of v
                nodes_out = np.flatnonzero(np.maximum(cpdag[v], 0))
                for w in nodes_out:
                    #if there exists w-u, orient it as u->w
                    if cpdag[w, u] == -1:
                        cpdag[w, u] = 0
                        cpdag[u, w] = 1

                        edges_found = True
                        if((u,w)) not in latest_edges_temp:
                            latest_edges_temp.append((u, w))
            #for R3 take any recently learnt edges and try to fit the pattern around it
            if not skip_r3:
                for edge in latest_edges:
                    v = edge[1]
                    u = edge[0]
                    #u is bottom left corner, v bottom right corner
                    #go over all colliders with it
                    nodes_collide_from = np.flatnonzero(np.maximum(cpdag[:, v], 0))
                    #w is toop right corner
                    for w in nodes_collide_from:
                        if w ==  u: #ignore the edge we already have
                            continue
                        #go over all possible diagonals
                        #t is top left node
                        for t in np.flatnonzero(np.minimum(cpdag[:, u], 0)):
                            #don't consider nodes already in the pattern
                            if t in [u, w, v]:
                                continue
                            #check if we get two triangles with new edges undirected
                            #also ensure the diagonal itself is= undirected
                            if (cpdag[t, u] == -1) and (cpdag[t, w] == -1) and (cpdag[t, v]==-1) and (cpdag[u, w] == 0) and (cpdag[w, u] == 0):

                                cpdag[t, v] = 1 #direct the diagonal
                                cpdag[v, t] = 0
                                edges_found = True
                                if((t,v)) not in latest_edges_temp:
                                    latest_edges_temp.append((t, v))

            #for R4 go over the case of detecting each directed edge separately
            #left side directed considered first
            for edge in latest_edges:
                v = edge[1]
                u = edge[0]
                #find the bottom directed edge that starts at v
                nodes_out = np.flatnonzero(np.maximum(cpdag[v], 0))
                if len(nodes_out) == 0:
                    continue
                poss_diags_in = np.flatnonzero(np.maximum(cpdag[:, v], 0)).tolist()
                poss_diags_out = np.flatnonzero(np.maximum(cpdag[v], 0)).tolist()
                poss_diags_undirected = np.flatnonzero(np.minimum(cpdag[:, v], 0)).tolist()

                for w in nodes_out:
                    #now find all diagionals (exclude case of edges identified)

                    #t is top right node
                    for t in (poss_diags_in + poss_diags_out + poss_diags_undirected):
                        if t in [u, w]:
                            continue
                        #check if the two undirected edges exist then orient the right one
                        if (cpdag[t, u] == -1) and (cpdag[t, w] == -1) and (cpdag[u, w] ==0) and (cpdag[w,u] == 0):
                            cpdag[t, w] = 1
                            cpdag[w, t] = 0

                            edges_found = True
                            if((t,w)) not in latest_edges_temp:
                                latest_edges_temp.append((t, w))

            #now consider its the bottom side we directed.

            for edge in latest_edges:
                v = edge[0]
                w = edge[1] #keep same names as for the code above

                nodes_in = np.flatnonzero(np.maximum(cpdag[:, v], 0))
                if len(nodes_in) == 0:
                    continue

                poss_diags_in = np.flatnonzero(np.maximum(cpdag[:, v], 0))
                poss_diags_out = np.flatnonzero(np.maximum(cpdag[v], 0))
                poss_diags_undirected = np.flatnonzero(np.minimum(cpdag[:, v], 0))
                for u in nodes_in:
                    #now everyting else is same as above
                    #now find all diagionals (exclude case of edges identified)

                    #t is top right node
                    for t in (poss_diags_in.tolist() + poss_diags_out.tolist() + poss_diags_undirected.tolist()):
                        if t in [u, w]:
                            continue
                        #check if the two undirected edges exist then orient the right one
                        if (cpdag[t, u] == -1) and (cpdag[t, w] == -1) and (cpdag[u,w] == 0) and (cpdag[w,u] == 0):
                            cpdag[t, w] = 1
                            cpdag[w, t] = 0
                            edges_found = True
                            if((t,w)) not in latest_edges_temp:
                                latest_edges_temp.append((t, w))

        latest_edges = latest_edges_temp

    return cpdag

def orient_from_intervention(dag, cpdag, intervention_set, is_tree=False):
    """
    uses the meek rules to update a cpdag given infinite sample interventional data
    from a single intervention. currently no implementation of soft interventions.
    input:
    matrix dag: the true dag
    matrix cpdag: the current cpdag
    list of lists of ints intervention_set: the nodes intervened on
    output:
    matrix: the cpdag after the intervention
    """

    n = dag.shape[0]
    new_edges = []
    for intervention in intervention_set:
        start_time = time.time()
        #first, orient all the edges we get from R0
        for v in intervention:
            for i in range(0, n):
                if i not in intervention:
                    #if its not an orientable edge, move on
                    if cpdag[v][i] != -1:
                        continue
                    cpdag[v][i] = dag[v][i]
                    cpdag[i][v] = dag[i][v]
                    if dag[v][i] == 1:
                        new_edges.append((v, i))
                    if dag[i][v] == 1:
                        new_edges.append((i, v))
    #second, extend the cpdag to a maximally oriented cpdag using the meek rules
    cpdag = meek(cpdag,new_edges, skip_r3 = False, is_tree=is_tree)
    return cpdag

def cpdag_from_dag_observational(dag):
    """
    given the true dag, returns the pcdag identified with just observational data

    input:
    matrix dag: the true dag

    output:
    matrix: the maximally oriented cpdag given just observational data
    """

    n = dag.shape[0]

    #first need to get the skeleton of the dag
    skeleton = -dag - dag.T
    cpdag = skeleton

    #second we identify all unshielded colliders in the true dag

    #for all nodes
    for i in range(0,n):
        #iterate over pairs of parents j,k
        parents_i = np.flatnonzero(dag[:,i])
        for j,k in itertools.combinations(parents_i, r=2):
            #if the parents are not adjacent we learn j->i and k->i
            if skeleton[j][k] == 0:
                cpdag[i][j] = 0
                cpdag[i][k] = 0
                cpdag[j][i] = 1
                cpdag[k][i] = 1

    #third we apply meek rules which are efficient in the observational setting
    cpdag = meek(cpdag)

    return cpdag

def cpdag_obj_val(cpdag, edgeorient=True):
    """
    computes the obj value of the cpdag. If edgeorient=True use the edge orienting objective
    input:
    matrix cpdag
    bool edgorient: whether to use the edgeorienting objective
    output:
    int: the objective value
    """

    return len(extract_all_directed(cpdag))

def orient_tree_root_v(cpdag, v):
    """
    orients a cpdag completely given the root. only true output for tree mecs: otherwise nonsense
    input:
    matrix cpdag
    int v: vertex to be the root
    output:
    matrix: a dag
    """
    n = cpdag.shape[0]
    #until we have oriented everything just go through applying R1
    oriented = [v]
    while oriented != []:
        u = oriented.pop()
        for i in range(0, n):
            if cpdag[u][i] == -1:
                cpdag[u][i] = 1
                cpdag[i][u] = 0
                oriented.append(i)
    return cpdag

def objective_given_intervention(cpdag, intervention_set, ref_cpdag, n_samples = 1, max_score=1, is_tree=False):
    """
    compute the objective value of a specific intervention
    input:
    matrix: cpdag
    list of lists of ints intervention_set
    matrix ref_cpdag: the cpdag used in the obj to count the number of oriented edges
    in a tree.
    int n_samples: number of samples if mode is "sample"
    output:
    int: the objective value of that intervention
    """
    #sum over all dags in the equivalence class
    n = cpdag.shape[0]
    obj = 0

    if mode == "sample":
        for dag in mec_size.uniform_sample_dag_plural(cpdag, n_samples):
            cpdag2 = orient_from_intervention(dag, cpdag.copy(), intervention_set, is_tree=is_tree)
            obj += (cpdag_obj_val(cpdag2) - cpdag_obj_val(ref_cpdag))/n_samples
        return obj


    return obj/max_score

def objective_given_dags_interventions(cpdag, intervention_set, ref_cpdag, dag_list, is_tree=False):
    """
    objective_given_intervention but takes in a fixed list of dags to eval on
    """
    obj = 0
    for dag in dag_list:
        cpdag2 = orient_from_intervention(dag, cpdag.copy(), intervention_set, is_tree=is_tree)
        obj += (cpdag_obj_val(cpdag2) - cpdag_obj_val(ref_cpdag))/len(dag_list)
    return obj

def gen_stochastic_grad_fun(cpdag, ref_cpdag, num_sample=1, exact=True, total_x = 1, is_tree=False):
    #generates the gradient function for a bag of cpdags
    #uses the edge orienting obj
    n = cpdag.shape[0]
    def stochastic_grad(intervention_set, x):
        """
        sample one dag in the sum, compute the stochastic gradient as in karimi et al 2018
        (for the multilinear extension)
        input:
        matrix: cpdag
        list of lists of ints intervention_set- existing interventions
        matrix ref_cpdag: the cpdag used in the obj to count the number of oriented edges
        array of floats x: the point we are taking the gradient at
        int num_sample: the number of samples used to approximate the objective
        bool exact, True if use exact uniform sampling, but false if use the fast inexact method
        int total_x: for each dag, how many different interventions do we try
        output:
        int: the gradient of the objective
        """

        grad_f = np.zeros(n)
        #sample the intervention given x

        dags = mec_size.uniform_sample_dag_plural(cpdag, num_sample, exact=exact)
        for dag in dags:
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                x_rand = np.random.binomial(1, p = x)

                for v in range(0, n):
                    x_rand_upper = x_rand.copy()
                    x_rand_upper[v] = 1
                    x_rand_lower = x_rand.copy()
                    x_rand_lower[v] = 0

                    #tobytes allows us to store the numpy array
                    if x_rand_upper.tobytes() not in computed_val:
                        cpdag_upper = orient_from_intervention(dag, cpdag.copy(), intervention_set+[np.flatnonzero(x_rand_upper).tolist()], is_tree=is_tree)
                        cpdag_upper_score = cpdag_obj_val(cpdag_upper)
                        computed_val[x_rand_upper.tobytes()] = cpdag_upper_score
                    else:
                        cpdag_upper_score  = computed_val[x_rand_upper.tobytes()]

                    if x_rand_lower.tobytes() not in computed_val:
                        cpdag_lower = orient_from_intervention(dag, cpdag.copy(), intervention_set+[np.flatnonzero(x_rand_lower).tolist()], is_tree=is_tree)
                        cpdag_lower_score = cpdag_obj_val(cpdag_lower)
                        computed_val[x_rand_lower.tobytes()] = cpdag_lower_score
                    else:
                        cpdag_lower_score  = computed_val[x_rand_lower.tobytes()]

                    grad_f[v] += (cpdag_upper_score - cpdag_lower_score)/ (num_sample*total_x)

        return grad_f
    return stochastic_grad

def gen_ss_stochastic_grad_fun(cpdag, ref_cpdag, num_sample=1, exact=True, total_x = 1, is_tree=False):
    #Same as gen_stochastic_grad_fun but sep system is the groundset and we add interventions
    #not perturbations. Uses the edge orienting objective
    n = cpdag.shape[0]
    def stochastic_grad(x, ss):
        """
        sample one dag in the sum, compute the stochastic gradient as in karimi et al 2018
        (for the multilinear extension)
        input:
        matrix: cpdag
        matrix ref_cpdag: the cpdag used in the obj to count the number of oriented edges
        array of floats x: the point we are taking the gradient at
        int num_sample: the number of samples used to approximate the objective
        bool exact, True if use exact uniform sampling, but false if use the fast inexact method
        int total_x: for each dag, how many different interventions do we try
        output:
        int: the gradient of the objective
        """
        n_ss = len(ss)

        grad_f = np.zeros(n_ss)
        #sample the intervention given x

        dags = mec_size.uniform_sample_dag_plural(cpdag, num_sample, exact=exact)
        for dag in dags:
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                x_rand = np.random.binomial(1, p = x)

                for v in range(0, n_ss):
                    x_rand_upper = x_rand.copy()
                    x_rand_upper[v] = 1
                    x_rand_lower = x_rand.copy()
                    x_rand_lower[v] = 0

                    #tobytes allows us to store the numpy array in a hashable way
                    if x_rand_upper.tobytes() not in computed_val:
                        indices = np.flatnonzero(x_rand_upper).tolist()
                        interventions = [ss[i] for i in indices]
                        cpdag_upper = orient_from_intervention(dag, cpdag.copy(), interventions, is_tree=is_tree)
                        cpdag_upper_score = cpdag_obj_val(cpdag_upper)
                        computed_val[x_rand_upper.tobytes()] = cpdag_upper_score
                    else:
                        cpdag_upper_score  = computed_val[x_rand_upper.tobytes()]

                    if x_rand_lower.tobytes() not in computed_val:
                        indices = np.flatnonzero(x_rand_lower).tolist()
                        interventions = [ss[i] for i in indices]
                        cpdag_lower = orient_from_intervention(dag, cpdag.copy(), interventions, is_tree=is_tree)
                        cpdag_lower_score = cpdag_obj_val(cpdag_lower)
                        computed_val[x_rand_lower.tobytes()] = cpdag_lower_score
                    else:
                        cpdag_lower_score  = computed_val[x_rand_lower.tobytes()]

                    grad_f[v] += (cpdag_upper_score - cpdag_lower_score)/ (num_sample*total_x)

        return grad_f
    return stochastic_grad

def pipage(x, k):
    """
    perform pipage rounding for a nonmonotone submodular function
    "Maximizing a Monotone Submodular Function subject to a Matroid Constraint"
    pipage round is performed on the set until 1 non integer value remains
    we will just randomly round the last value by sampling a bernoulli
    input:
    float array x: the solution to be rounded
    int k: constraint on perturbation size

    output:
    int list: the intervention
    """
    x = np.round(x, decimals=10) #round to 10 decimals for numerical stability

    #now select noninteger entries given by T as is the notation in the paper above
    T = np.flatnonzero((np.round(x, decimals=0) - x)).tolist()

    epsilon = 0.001 #rounding threshold for numerical stability
    while len(T) > 1:
        ij = np.random.choice(T, 2, replace=False)
        i = ij[0]
        j = ij[1]
        #option 1: x_i is the one where we make the i direction large
        dis_to_boundary_i = np.minimum(1-x[i], x[j])
        x_i = x.copy()
        x_i[i] += dis_to_boundary_i
        x_i[j] -= dis_to_boundary_i

        #same for j

        dis_to_boundary_j = np.minimum(1-x[j], x[i])
        x_j = x.copy()
        x_j[i] -= dis_to_boundary_j
        x_j[j] += dis_to_boundary_j

        #sample the direction based on the sampling rule
        p = abs(dis_to_boundary_i / (dis_to_boundary_j + dis_to_boundary_i))
        if p>1+epsilon:
            print(x)
            print(p)
            print(x_i)
            print(x_j)
        assert p < 1 + epsilon #check that we don't have funky probabilities

        if p < 1+ epsilon and p > 1:
            p = 1
        if p > - epsilon and p < 0:
            p = 0

        change_lower = np.random.binomial(1, p, 1)[0]
        if change_lower:
            x = x_j
        else:
            x = x_i

        #change T if we get to some integer solutions
        if abs(x[i] - 1) < epsilon or abs(x[i]) < epsilon:
            x[i] = round(x[i])
            T.remove(i)
        if abs(x[j] - 1) < epsilon or abs(x[j]) < epsilon:
            x[j] = round(x[j])
            T.remove(j)

    #if one element left, bernoulli sample its value. can do since submod->concave along
    #any nonnegative direction vector
    if len(T) == 1:
        #first just set to 0 if fulfilled condition and have rounding error
        if np.sum(x) > k and np.sum(x) < k+0.1:
            x[T[0]] = 0
        else:
            #if no interventions yet just do the intervention for sure
            if(np.sum(x) <= 1):
                x[T[0]] = 1
            #otherwise sample whether to include
            else:
                x[T[0]] = np.random.binomial(1, x[T[0]], 1)[0]

    #If we get[0.9, 0, 0], we can intervene on nothing. The fixed used is to
    #intervene on the last remaining variable for sure. That's in the if above
    return x

def gen_hess_fun(cpdag, ref_cpdag, num_sample=1, exact=True, total_x = 1, is_tree=False):
    """
    generates the hessian function of the edge orienting obj given a bag of cpdags
    num_sample: number of times we sample a dag
    total_x: number of repeats per sampled dag
    """
    n = cpdag.shape[0]
    def hess_fun(intervention_set, x, e):
        """
        estimates the hessian for gred
        """
        #sample the intervention given x

        dags = mec_size.uniform_sample_dag_plural(cpdag, num_sample, exact=exact)
        hess = np.zeros((n, n))
        for dag in dags:
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                S = []
                for s in range(n):
                    if e[s] < x[s]:
                        S.append(s)
                for i in range(n):
                    for j in range(i, n):
                        if i == j:
                            continue
                        S_ij = list({i, j}.union(set(S)))
                        S_i = list({i}.union(set(S)) - {j})
                        S_j = list({j}.union(set(S)) - {i})
                        S_minus = list(set(S) - {i,j}) #the set with both indices removed
                        for S_mod in [S_ij, S_i, S_j, S_minus]:
                            if np.array(S_mod).tobytes() not in computed_val:
                                cpdag_new = orient_from_intervention(dag, cpdag.copy(), intervention_set+[S_mod], is_tree=is_tree)
                                computed_val[np.array(S_mod).tobytes()] = cpdag_obj_val(cpdag_new)

                        hess[i,j] += (computed_val[np.array(S_ij).tobytes()]-computed_val[np.array(S_i).tobytes()]-
                            computed_val[np.array(S_j).tobytes()]+computed_val[np.array(S_minus).tobytes()])/ (num_sample*total_x)
            #print(time.time()-time2)
        return hess
    return hess_fun

def gen_ss_hess_fun(cpdag, ref_cpdag, num_sample=1, exact=True, total_x = 1, is_tree=False):
    """
    hessian for SS-based approach
    """
    n = cpdag.shape[0]
    def hess_fun(x, e, ss):
        """
        estimates the hessian for gred
        """
        n_ss=len(ss)

        #sample the intervention given x

        dags = mec_size.uniform_sample_dag_plural(cpdag, num_sample, exact=exact)
        hess = np.zeros((n_ss, n_ss))
        for dag in dags:
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                S = []
                for s in range(n_ss):
                    if e[s] < x[s]:
                        S.append(ss[s])
                for i in range(n_ss):
                    for j in range(i, n_ss):
                        if i == j:
                            continue
                        S_ij = list({tuple(a) for a in ([ss[i], ss[j]] + S)})
                        S_i = list({tuple(a) for a in ([ss[i]] + S)} - {tuple(ss[j])})
                        S_j = list({tuple(a) for a in ([ss[j]] + S)} - {tuple(ss[i])})
                        S_minus = list({tuple(a) for a in S} - {tuple(ss[j]), tuple(ss[i])}) #the set with both indices removed
                        for S_mod in [S_ij, S_i, S_j, S_minus]:
                            if np.array(S_mod).tobytes() not in computed_val:
                                cpdag_new = orient_from_intervention(dag, cpdag.copy(), S_mod, is_tree=is_tree)
                                computed_val[np.array(S_mod).tobytes()] = cpdag_obj_val(cpdag_new)

                        hess[i,j] += (computed_val[np.array(S_ij).tobytes()]-computed_val[np.array(S_i).tobytes()]-
                            computed_val[np.array(S_j).tobytes()]+computed_val[np.array(S_minus).tobytes()])/ (num_sample*total_x)
        return hess
    return hess_fun

def scdpp(cpdag, n_b, k, obj, stochastic_grad, hess_fun, T=100, max_score=1, M0 = 10, M=10, num_dag_sample=10, is_tree=False):
    """
    The improved version of stochastic coninuous greedy by Hassani et al 2020
    uses hessian approximation instead of just gradient information
    M0 and M are minibatch sizes
    """
    intervention_set = []
    n = cpdag.shape[0]

    u_bar = 1 #the upper bound for each variable in x_t
    A = np.vstack((np.ones(n), np.eye(n)))  #constraint matrix for linear program

    #gen_stochastic_grad_function needs num_x to be set to M
    for _ in range(n_b):
        #first lets do t=1
        x_t = np.zeros(n)
        x_tp = np.zeros(n) #previous x
        grad_f = np.zeros(n)
        for t in range(1, T):
            if t == 1:
                #sample minibatch and compute gradient g^0
                for i in range(M0):
                    grad_f += stochastic_grad(intervention_set, x_t) / M0
            else:
                hess = np.zeros((n,n))
                for i in range(M):
                    a = np.random.uniform()
                    e = np.random.uniform(size=n)
                    x_a = a * x_t + (1-a) * x_tp
                    hess += hess_fun(intervention_set, x_a, e) / M
                delta = hess.dot(x_t - x_tp)
                grad_f = grad_f + delta
            #compute ascent direction
            b = np.insert(u_bar-x_t, 0, k) #constraint vector
            v_t = scipy.optimize.linprog(-grad_f, A_ub = A, b_ub = b, bounds = (0,1))
            x_tp = x_t
            x_t = x_t + v_t.x / T

        if(np.sum(x_t) > k+0.1):
            raise Exception
        #now do pipage rounding a few times and pick the best
        best_x = pipage(x_t, k)
        best_score = obj(intervention_set+[np.flatnonzero(best_x).tolist()])
        #do ten runs of pipage rounding and pick the best
        for _ in range(10):
            x = pipage(x_t, k)
            x_score = obj(intervention_set+[np.flatnonzero(x).tolist()])
            if x_score > best_score:
                best_x = x
                best_score = x_score
        intervention_set.append(np.flatnonzero(best_x).tolist())

    return intervention_set

def scdpp_ss(cpdag, n_b, k, obj, stochastic_grad, hess_fun, T=100, max_score=1, M0 = 10, M=10, num_dag_sample=10, is_tree=False, all_k = True, smart_ss=True):
    """
    The improved version of monotone stochastic coninuous greedy by Hassani et al 2020
    M0 and M are minibatch sizes.
    With seperating system to select perturbations, so just use continuous approach
    to select interventions from SS.
    """
    n = cpdag.shape[0]
    if smart_ss:
        ss = smart_ss_construct(cpdag, k)
    else:
        #will only do it for the k given
        ss = ss_construct(n, k)
    n_ss = len(ss)
    #if we can completely identify using the seperating system, do so
    if n_ss <= n_b:
        return ss

    A = np.ones((1, n_ss))   #constraint matrix for linear program

    #gen_stochastic_grad_function needs num_x to be set to M
    #first lets do t=1
    x_t = np.zeros(n_ss)
    x_tp = np.zeros(n_ss) #previous x
    grad_f = np.zeros(n_ss)
    for t in range(1, T):
        if t == 1:
            #sample minibatch and compute gradient g^0
            for i in range(M0):
                grad_f += stochastic_grad(x_t, ss) / M0
        else:
            hess = np.zeros((n_ss,n_ss))
            for i in range(M):
                a = np.random.uniform()
                e = np.random.uniform(size=n_ss)
                x_a = a * x_t + (1-a) * x_tp
                hess += hess_fun(x_a, e, ss) / M
            delta = hess.dot(x_t - x_tp)
            grad_f = grad_f + delta
        #compute ascent direction
        v_t = scipy.optimize.linprog(-grad_f, A_ub = A, b_ub = np.asarray([n_b]), bounds = (0,1))
        x_tp = x_t
        x_t = x_t + v_t.x / T

    #check x doesn't break the constraint
    if(np.sum(x_t) > n_b+0.1):
        raise Exception
    #resolve any slight numerical issues by ensuring the constraints are met
    x_t = np.minimum(x_t / np.linalg.norm(x_t, ord=1) * n_b, 1)
    #now do pipage rounding a few times and pick the best
    best_x = pipage(x_t, n_b)
    indices = np.flatnonzero(best_x).tolist()
    interventions = [ss[i] for i in indices]
    best_score = obj(interventions)
    #do ten runs of pipage rounding and pick the best
    for _ in range(10):
        x = pipage(x_t, n_b)
        indices = np.flatnonzero(x).tolist()
        interventions = [ss[i] for i in indices]
        x_score = obj(interventions)
        if x_score > best_score:
            best_x = x
            best_score = x_score

    indices = np.flatnonzero(best_x).tolist()
    interventions = [ss[i] for i in indices]

    return interventions

def edge_obj_sample(cpdags, ws, num_samples, obj=None, is_tree=False):
    """
    takes obj that uses a list of dags, and returns the obj with a fixed dag list
    cpdags is a list of MECs, ws is weights. We uniform sample from the possible DAGs
    If obj==None, assume is edge orienting objective
    """
    if obj in [objective_given_dags_interventions, None]:
        num_cpdags = len(cpdags)
        dag_list = []
        cpdag_list = []
        for i in range(num_samples):
            cpdag = cpdags[np.random.choice(num_cpdags, p=ws)]
            dag = mec_size.uniform_sample_dag_plural(cpdag, 1)[0]
            cpdag_list.append(cpdag)
            dag_list.append(dag)
        def new_obj(epsilon):
            out = 0
            for i in range(num_samples):
                out += objective_given_dags_interventions(cpdag_list[i], epsilon, cpdag_list[i].copy(), [dag_list[i]], is_tree=is_tree) / num_samples
            return out
        return new_obj
    return

def weighted_dags_edge_obj_sample(cpdags, ws, dags, obj = None, total_x=1, is_tree=False):
    """
    Just uses a list of given bootstrapped dags and corresponding cpdags and weights to
    construct an objective that uses the same dist over dags as the finite sample obj
    also returns a way for us to get a stochastic gradients function

    total x is the number of samples of the intervention from
    the categorical dist given by x_t when computing the stochastic grad
    """
    n = cpdags[0].shape[0]
    num_samples = len(cpdags)
    def new_obj(epsilon):
        out = 0
        for i in range(num_samples):
            out += objective_given_dags_interventions(cpdags[i], epsilon, cpdags[i].copy(), [dags[i]], is_tree=is_tree) * ws[i]
        return out
    def new_stochastic_grad(intervention_set, x):
        """
        intervention set is existing interventions, x is the continuous numpy array
        """
        grad_f = np.zeros(n)
        #sample the intervention given x
        indexes = np.random.randint((len(dags)), size=1)

        for i in indexes:
            dag = dags[i]
            cpdag = cpdags[i]
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                x_rand = np.random.binomial(1, p = x)

                for v in range(0, n):
                    x_rand_upper = x_rand.copy()
                    x_rand_upper[v] = 1
                    x_rand_lower = x_rand.copy()
                    x_rand_lower[v] = 0

                    #tobytes allows us to store the numpy array
                    if x_rand_upper.tobytes() not in computed_val:
                        cpdag_upper_score = new_obj(intervention_set+[np.flatnonzero(x_rand_upper).tolist()])
                        computed_val[x_rand_upper.tobytes()] = cpdag_upper_score
                    else:
                        cpdag_upper_score  = computed_val[x_rand_upper.tobytes()]

                    if x_rand_lower.tobytes() not in computed_val:
                        cpdag_lower_score = new_obj(intervention_set+[np.flatnonzero(x_rand_lower).tolist()])
                        computed_val[x_rand_lower.tobytes()] = cpdag_lower_score
                    else:
                        cpdag_lower_score  = computed_val[x_rand_lower.tobytes()]

                    grad_f[v] += ws[i] * (cpdag_upper_score - cpdag_lower_score)/ (total_x* len(indexes))

        return grad_f

    def hess_fun(intervention_set, x, e):
        """
        estimates the hessian for gred
        """
        #print(cpdag)

        indexes = np.random.randint((len(dags)), size=1)
        #sample the intervention given x

        #print("stochastic grad inner")
        hess = np.zeros((n, n))
        for ind in indexes:
            dag = dags[ind]
            cpdag = cpdags[ind]
            #time2 = time.time()
            #print(time2-time1)
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                S = []
                for s in range(n):
                    if e[s] < x[s]:
                        S.append(s)
                for i in range(n):
                    for j in range(i, n):
                        if i == j:
                            continue
                        S_ij = list({i, j}.union(set(S)))
                        S_i = list({i}.union(set(S)) - {j})
                        S_j = list({j}.union(set(S)) - {i})
                        S_minus = list(set(S) - {i,j}) #the set with both indices removed
                        for S_mod in [S_ij, S_i, S_j, S_minus]:
                            if np.array(S_mod).tobytes() not in computed_val:
                                cpdag_new = orient_from_intervention(dag, cpdag.copy(), intervention_set+[S_mod], is_tree=is_tree)
                                computed_val[np.array(S_mod).tobytes()] = cpdag_obj_val(cpdag_new)

                        hess[i,j] += ws[ind] * (computed_val[np.array(S_ij).tobytes()]-computed_val[np.array(S_i).tobytes()]-
                            computed_val[np.array(S_j).tobytes()]+computed_val[np.array(S_minus).tobytes()])/ (total_x* len(indexes))
            #print(time.time()-time2)
        return hess
    return new_obj, new_stochastic_grad, hess_fun

def weighted_dags_edge_obj_sample_ss(cpdags, ws, dags, obj = None, total_x=1, is_tree=False):
    """
    Just uses a list of given bootstrapped dags and corresponding cpdags and weights to
    construct an objective that uses the same dist over dags as the finite sample obj
    also returns a way for us to get a stochastic gradients function

    Work with both finite and infinite sample objective

    total x is the number of samples of the intervention from
    the categorical dist given by x_t when computing the stochastic grad
    """
    num_samples = len(cpdags)
    n = cpdags[0].shape[0]

    def new_obj(epsilon):
        out = 0
        if obj == "MI":
            out += scipy.stats.entropy(ws, base=2)
            new_cpdags = []
            #print(cpdags)
            for i in range(num_samples):
                new_cpdags.append(orient_from_intervention(dags[i], cpdags[i].copy(), epsilon, is_tree=is_tree))
            #orient all the dags as if true_G is the true DAG
            #add on w* entropy of resulting distribution
            for i in range(num_samples):
                new_ws = ws.copy()
                for j in range(num_samples):
                    if j == i:
                        continue
                    if not np.array_equal(cpdags[i], cpdags[j]):
                        new_ws[j] = 0
                        continue
                    if not np.array_equal(new_cpdags[i], new_cpdags[j]):
                        new_ws[j] = 0
                out += - ws[i] * scipy.stats.entropy(new_ws, base=2)
        else:
            for i in range(num_samples):
                out += objective_given_dags_interventions(cpdags[i], epsilon, cpdags[i].copy(), [dags[i]], is_tree=is_tree) * ws[i]
        return out
    def new_stochastic_grad(x, ss):
        """
        intervention set is existing interventions, x is the continuous numpy array
        """

        #sample the intervention given x
        n_ss = len(ss)

        grad_f = np.zeros(n_ss)

        indexes = np.random.randint((len(dags)), size=1)
        #sample the intervention given x
        for ind in indexes:
            dag = dags[ind]
            cpdag = cpdags[ind]
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                x_rand = np.random.binomial(1, p = x)

                for v in range(0, n_ss):
                    x_rand_upper = x_rand.copy()
                    x_rand_upper[v] = 1
                    x_rand_lower = x_rand.copy()
                    x_rand_lower[v] = 0

                    #tobytes allows us to store the numpy array
                    if x_rand_upper.tobytes() not in computed_val:
                        indices = np.flatnonzero(x_rand_upper).tolist()
                        interventions = [ss[i] for i in indices]
                        cpdag_upper_score = objective_given_dags_interventions(cpdag, interventions, cpdag.copy(), [dag], is_tree=is_tree)
                        computed_val[x_rand_upper.tobytes()] = cpdag_upper_score
                    else:
                        cpdag_upper_score  = computed_val[x_rand_upper.tobytes()]

                    if x_rand_lower.tobytes() not in computed_val:
                        indices = np.flatnonzero(x_rand_lower).tolist()
                        interventions = [ss[i] for i in indices]
                        cpdag_lower_score = objective_given_dags_interventions(cpdag, interventions, cpdag.copy(), [dag], is_tree=is_tree)
                        computed_val[x_rand_lower.tobytes()] = cpdag_lower_score
                    else:
                        cpdag_lower_score  = computed_val[x_rand_lower.tobytes()]

                    grad_f[v] += ws[ind] * (cpdag_upper_score - cpdag_lower_score)/ (total_x * len(indexes))

        return grad_f

    def hess_fun(x, e, ss):
        """
        estimates the hessian for gred
        """
        n_ss=len(ss)

        grad_f = np.zeros(n)
        #sample the intervention given x

        hess = np.zeros((n_ss, n_ss))
        indexes = np.random.randint((len(dags)), size=1)
        #sample the intervention given x

        for ind in indexes:
            dag = dags[ind]
            cpdag = cpdags[ind]
            computed_val = {}
            #do runs for multiple different samples of x
            for _ in range(total_x):
                S = []
                for s in range(n_ss):
                    if e[s] < x[s]:
                        S.append(ss[s])
                for i in range(n_ss):
                    for j in range(i, n_ss):
                        if i == j:
                            continue
                        S_ij = list({tuple(a) for a in ([ss[i], ss[j]] + S)})
                        S_i = list({tuple(a) for a in ([ss[i]] + S)} - {tuple(ss[j])})
                        S_j = list({tuple(a) for a in ([ss[j]] + S)} - {tuple(ss[i])})
                        S_minus = list({tuple(a) for a in S} - {tuple(ss[j]), tuple(ss[i])}) #the set with both indices removed
                        for S_mod in [S_ij, S_i, S_j, S_minus]:
                            if np.array(S_mod).tobytes() not in computed_val:
                                cpdag_new = orient_from_intervention(dag, cpdag.copy(), S_mod, is_tree=is_tree)
                                computed_val[np.array(S_mod).tobytes()] = cpdag_obj_val(cpdag_new)

                        hess[i,j] += ws[ind] * (computed_val[np.array(S_ij).tobytes()]-computed_val[np.array(S_i).tobytes()]-
                            computed_val[np.array(S_j).tobytes()]+computed_val[np.array(S_minus).tobytes()])/ (total_x * len(indexes))
        return hess
    return new_obj, new_stochastic_grad, hess_fun

def ss_construct(n, k):
    """
    Given the size of the graph, constructs a separating system agnostic to the
    structure of the graph. Effectively implements theorem 1 in Shanmugam 2015
    input:
    int n
    int k
    output:
    list of list of ints separating system
    """
    a = math.ceil(n/k)
    l = math.ceil(math.log(n, a))
    #get distinct 'l' length labels for all elements 1:n, using letter from alphabet size 'a'+1
    #in every digit position any integer is used at most 'a' times

    #labels is n lists each with l elements
    labels = []

    x = l - 1
    for i in range(0, n):
        labels.append([])
    for d_ind in range(0, x+1):
        d = d_ind + 1

        p_d, r_d = divmod(n, (math.pow(a, d)))
        p_d1, r_d1 = divmod(n, (math.pow(a, d_ind)))

        #step one in the lemma
        count = 0
        num = 0
        while count < p_d * math.pow(a, d):
            #ensure we dopn't overflow the number to append
            amount_append = int(min(p_d * math.pow(a, d) - count, math.pow(a, d_ind)))
            for _ in range(amount_append):
                labels[count].append(num)
                count += 1
            num += 1
            if num > a-1:
                num = 0
        #step 2 in the lemma. bit unclear but i think it means go 0, 1, a-1 each r_d/a times
        num=0
        while count < n:
            amount_append = int(min(math.ceil(r_d/a), n - count))
            for _ in range(amount_append):
                labels[count].append(num)
                count+= 1
            num += 1

        #step 3 in the lemma
        for i in range(int(math.pow(a, d_ind)*p_d1), n):
            labels[i][-1] += 1 #increase last element of the list

    ss = []
    for i in range(1, l+1):
        for j in range(0, a):
            s_ij = []
            for label_ind in range(len(labels)):
                label = labels[label_ind]
                if label[i-1] == j:
                    s_ij.append(label_ind)
            ss.append(s_ij)

    return ss

def smart_ss_construct(cpdag, k):
    """
    Takes in the cpdag, k, and returns a seperating system as
    given in Lindgren et al. 18.
    For finding minimum vertex cover we use a greedy algorithm,
    For finding an optimal coloring we use welch-powell.
    """


    n = cpdag.shape[0]

    G = cpdag.copy()

    #convert the cpdag to a chordal graph
    #remove the directed edges
    for i in range(n):
        for j in range(i, n):
            if G[i][j] != G[j][i]:
                G[i][j] = 0
                G[j][i] = 0

    #now construct an approximately minimal vertex cover
    Gx = nx.DiGraph(G)
    S = vertex_cover.min_weighted_vertex_cover(Gx)
    #construct a coloring of the graph induced by the vertex cover
    Gs = nx.subgraph(Gx, S)

    coloring = nx.greedy_color(Gs, strategy='largest_first')

    #from each color, select intervention of size at most k
    def chunks(l, k):
        #chunks up a list l into pieces of size at most k
        assert k > 0
        return [l[i:i+k] for i in range(0, len(l), k)]

    interventions = []
    all_colors = set(coloring.values()) #set removes repeats
    for color in all_colors:
        l = [k for k,v in coloring.items() if v == color]
        interventions = interventions + chunks(l, k)

    return interventions

def lazy_ss_intervention(n, n_b, k, obj, cpdag, smart_ss = True, all_k = True, verbose=False):
    """
    greedily selects interventions from a seperating system but uses lazy evaluation for speedup
    input:
    n: number nodes in the true DAG
    int n_b: batch size
    int k: intervention size
    int num_sample: the number of samples used to approximate the objective
    function obj: an objective function.
    matrix cpdag:
    smart_ss bool: is True if using a graph specific ss constructor
    bool all_k: if to try the SS for all or
    """

    best_interventions = []
    best_score = -np.inf
    #iterate over possible k and then pick the best at the end
    for k_cand in range(1, k+1):
        #first construct a separating system. we use Shanmugam 2015 which is agnostic
        #to the strucxture of the graph
        interventions = []
        if smart_ss:
            k_cand = k #skip all other possible k
            ss = smart_ss_construct(cpdag, k_cand)
        else:
            if not all_k:
                k_cand=k
            ss = ss_construct(n, k_cand)

        #return some random interventions if the separating system is empty
        if len(ss) == 0:
            return [np.random.randint(n, size=k).tolist() for _ in range(n_b)]

        cur_obj = obj

        delta_v = np.zeros(len(ss)) + np.inf #start initing at infty for the deltas

        current_batch_score = 0 #the score of the current batch

        #for each element in the final batch choose greedily
        for i in range(n_b):

            #sort delta_v from largest to smallest

            best_intervention = []
            rel_improv = -np.inf
            #work from the current best intervention
            for j in np.flip(np.argsort(delta_v)):
                #print(j)
                intervention_cand = ss[j]
                #if you already have the intervention you can just skip it
                #removed this since in finite samples this isnt true
                """
                if intervention_cand in interventions:
                    delta_v[j] = -np.inf
                    continue
                """
                interventions_cand = interventions + [intervention_cand]

                #print(interventions_cand)
                cand_score = cur_obj(interventions_cand)
                #print(cand_score)

                rel_improv_j = cand_score - current_batch_score
                #print(rel_improv_j)
                if verbose:
                    print(interventions_cand)
                    print(cand_score)

                delta_v[j] = rel_improv_j
                #if its better than everything else already, break

                #print(rel_improv_j)
                #print(delta_v)
                #print(j)
                if rel_improv_j >= np.max(delta_v):
                    break

            #of the best interventions choose randomly
            if np.flatnonzero(delta_v == delta_v.max()).size == 0:
                print(delta_v)
                print(cand_score)
                print(ss)
                print(cur_obj(interventions_cand))
            best_intervention_index = np.random.choice(np.flatnonzero(delta_v == delta_v.max()))
            current_batch_score = delta_v[best_intervention_index] + current_batch_score

            interventions.append(ss[best_intervention_index])

        score = cur_obj(interventions)
        if verbose:
            print(interventions)
            print(score)
        #now greedily sample an intervention from it and save its obj if its better
        if score > best_score:
            best_score = score
            best_interventions = interventions
    #pick the intervention set with best score
    return best_interventions

def lazy_drg(n, n_b, k, obj, verbose=False):
    """
    An approach to our stochastic continuous gredy algorithm that uses a fixed objective
    and the discrete random greedy algorithm
    (https://theory.epfl.ch/moranfe/Publications/SODA2014.pdf) to select interventions.
    """
    #take the k elements with the highest marginal improvement
    #sample from these uniformly
    best_interventions = []
    best_score = -np.inf

    interventions = []

    cur_obj = obj


    #for each element in the final batch choose greedily
    for i in range(n_b):

        #sort delta_v from largest to smallest
        rel_improv = -np.inf

        #last 2k elements have dummy and contribute 0 at any point
        delta_v = np.zeros(n+2*k) + np.inf #start initing at infty for the deltas

        current_batch_score = 0 #the score of the current batch
        intervention = []
        for _ in range(k):

            #work from the current best intervention
            num_updated = 0
            for j in np.flip(np.argsort(delta_v)):

                #if updated more than k do a lazy check
                if num_updated >= k:
                    #break if there are k options with better marginal improvement
                    if np.flip(np.argsort(delta_v)[k-1]) > delta_v[j]:
                        break

                if j in intervention:
                    cand_score = -np.inf
                if j >= n:
                    cand_score = current_batch_score #marginal cont of dummy vars is 0
                    num_updated+=1
                else:
                    interventions_cand = interventions + [intervention + [j]]
                    cand_score = cur_obj(interventions_cand)
                    num_updated+=1

                    if verbose:
                        print(interventions_cand)
                        print(cand_score)


                rel_improv_j = cand_score - current_batch_score


                delta_v[j] = rel_improv_j
                #if its better than everything else already, break
                #once we have k elements better than the best previous gain we can stop


            #from the k best interventions sample uniformly
            best_intervention_index = np.flip(np.argsort(delta_v))[np.random.randint(0, k)]
            current_batch_score = delta_v[best_intervention_index] + current_batch_score

            if best_intervention_index < n: #only add if not a dummy variable
                intervention.append(best_intervention_index)
        interventions.append(intervention)

    score = cur_obj(interventions)
    if verbose:
        print(interventions)
        print(score)
    #pick the intervention set with best score
    return interventions

def process_ov(inter, b, k, OVs, obj, meth, k_range):
    """
    process experiment runs by feeding them into obj value list
    """
    f = "b=" + str(b) + '_k=' + str(k) + "_" + meth
    #for ss_a, might favour smaller seperating system if est objective is higher
    if meth in ['ss_a', 'ss_a_cont']:
        for kp in k_range:
            if kp < k:
                fp = "b=" + str(b) + '_k=' + str(kp) + "_" + meth
                if OVs[fp][-1] > obj:
                    obj = OVs[fp][-1]
    OVs[f].append(obj)
    return

def run_experiment(n, generator, meths, labs, k_range, title = '', name='', repeats=10):
    """
    runs experiments on chain graphs
    input:
    int n: number of nodes in chain
    str generator: a str saying what generator to use: 'chain', 'tree'...
    str title: the plot title
    str name: the plot filename
    int repeats: number of repetitions
    output:
    saves a plot of the results
    """

    #it doesn't really matter that we change the root since the objective and method
    #only see the MEC. it will matter when we do non-trees since can't sample whole mec
    fig = plt.figure()
    b_range = [1, 2, 3, 4, 5]
    plt.xticks(b_range)

    lines = ['-', '--', ':']
    invalid_list = []

    OVs = {}
    times_dict = {}

    for _ in range(repeats):
        valid_dag = False
        while valid_dag == False:
            random_root = np.random.choice(n)
            if generator == 'chain':
                dag = generate_chain_dag_fixed_root(n, random_root)
            elif generator == 'tree':
                dag = uniform_random_tree(n)
            elif generator == "bipartite":
                dag = ER_bipartite(int(n/2), n- int(n/2), 0.5)
            elif generator.startswith('ER'):
                #for erdos renyi write ER plus the param value
                rho = float(generator.split('_')[1])
                dag=generate_ER(n, rho)
            elif generator == "fully_connected":
                dag = generate_fully_connected(n)
            elif generator.startswith("barabasi_albert"):
                #for barabasi-albert append "_m" to the string
                m = generator.split('_')[2]
                dag = generate_barabasi_albert(n, m)
            elif generator == "kstar":
                dag = generate_k_star_system(n, max(k_range))
            elif generator.startswith("dream"):
                #read in the dream dag
                exp = int(generator.split("_")[1])
                cells = ["Ecoli1", "Ecoli2", "Yeast1", "Yeast2", "Yeast3"]
                f =  "gnw_obs/InSilicoSize50-" + cells[exp-1] + "_goldstandard_signed.tsv"
                dag = dream.load_true_dag(50, f)
            else:
                raise Exception
            cpdag = cpdag_from_dag_observational(dag)
            max_score = cpdag_obj_val(dag) - cpdag_obj_val(cpdag) # for normalizing scores
            if generator.startswith("dream"):
                #no need to compute MEC size if in dream mode
                valid_dag=True
                break
            mec_size_total = mec_size.mec_size(cpdag)
            if generator not in ['fully_connected', 'chain', 'tree', 'kstar']:
                #reject DAGs that have MEC too small or too large (computational reasons)
                if n <= 20:
                    lower_const = 10
                else:
                    lower_const = 20
                if  mec_size_total <=100 and mec_size_total >= lower_const:
                    valid_dag = True
                else:
                    invalid_list.append(1)
            else:
                valid_dag = True

        if not generator.startswith("dream"):
            full_mec = mec_size.enumerate_dags(cpdag)

        #print(objective_given_intervention(cpdag, [[0]], cpdag.copy()))

        num_samples = 40

        if generator in ["tree", "chain", "kstar"]:
            is_tree=True
        else:
            is_tree=False

        for k in k_range:
            for b in b_range:
                for i in range(len(meths)):
                    meth = meths[i]
                    f = "b=" + str(b) + '_k=' + str(k) + "_" + meth
                    if f not in OVs:
                        OVs[f] = []
                        times_dict[f] = []

                    #do one run of greedy on last round
                    if meth in ['ss_a', 'ss_b', 'cont', 'drg'] and b != b_range[-1]:
                        continue
                    start_time = time.perf_counter()
                    if meth == 'rand':
                        inter = chordal_random_intervention_set(cpdag, b, k)
                    elif meth == 'ss_a':
                        ss_obj = edge_obj_sample([cpdag], [1], num_samples, is_tree=is_tree) #use weights as 1
                        inter = lazy_ss_intervention(cpdag.shape[0], b, k, ss_obj, cpdag, smart_ss=False, all_k=False)
                    elif meth == 'ss_b':
                        ss_obj = edge_obj_sample([cpdag], [1], num_samples, is_tree=is_tree) #use weights as 1
                        inter = lazy_ss_intervention(cpdag.shape[0], b, k, ss_obj, cpdag, smart_ss=True)
                    elif meth == 'ss_a_cont':
                        ss_obj = edge_obj_sample([cpdag], [1], num_samples, is_tree=is_tree)
                        ss_stochastic_grad = gen_ss_stochastic_grad_fun(cpdag, cpdag.copy(), num_sample=1, exact=False, total_x = 1, is_tree=is_tree)
                        hess_fun = gen_ss_hess_fun(cpdag, cpdag.copy(), num_sample=1, exact=False, total_x = 1, is_tree=is_tree)
                        #run b times longer
                        inter = scdpp_ss(cpdag, b, k, ss_obj, ss_stochastic_grad, hess_fun, T=5*b, max_score=1, M0 = 5, M=5, smart_ss=False)
                    elif meth == 'ss_b_cont':
                        ss_obj = edge_obj_sample([cpdag], [1], num_samples, is_tree=is_tree)
                        ss_stochastic_grad = gen_ss_stochastic_grad_fun(cpdag, cpdag.copy(), num_sample=1, exact=False, total_x = 1, is_tree=is_tree)
                        hess_fun = gen_ss_hess_fun(cpdag, cpdag.copy(), num_sample=1, exact=False, total_x = 1, is_tree=is_tree)
                        #run b times longer
                        inter = scdpp_ss(cpdag, b, k, ss_obj, ss_stochastic_grad, hess_fun, T=5*b, max_score=1, M0 = 5, M=5, smart_ss=True)
                    elif meth == 'cont':
                        gred_obj = edge_obj_sample([cpdag], [1], num_samples, is_tree=is_tree)
                        gred_stochastic_grad = gen_stochastic_grad_fun(cpdag, cpdag.copy(), num_sample=1, exact=False, total_x = 1, is_tree=is_tree)
                        hess_fun = gen_hess_fun(cpdag, cpdag.copy(), num_sample=1, exact=False, total_x = 1, is_tree=is_tree)
                        inter = scdpp(cpdag, b, k, gred_obj, gred_stochastic_grad, hess_fun, T=5, max_score=1, M0 = 5, M=5)
                    elif meth == 'drg':
                        drg_obj = edge_obj_sample([cpdag], [1], num_samples, is_tree=is_tree)
                        inter=lazy_drg(n, b, k, drg_obj)
                    else:
                        #raise exception if no function for that method
                        raise Exception
                    #store objective and time
                    times_dict[f].append(time.perf_counter()-start_time)
                    if generator.startswith("dream"):
                        obj = objective_given_dags_interventions(cpdag, inter, cpdag.copy(), [dag], is_tree=is_tree)/ max_score
                    else:
                        obj = objective_given_dags_interventions(cpdag, inter, cpdag.copy(), full_mec, is_tree=is_tree)/ max_score

                    if meth in ['ss_a', 'ss_b', 'cont', 'drg']:
                        for bp in b_range:
                            inter_p = inter[0:bp]
                            if generator.startswith("dream"):
                                obj_p = objective_given_dags_interventions(cpdag, inter_p, cpdag.copy(), [dag], is_tree=is_tree)/ max_score
                            else:
                                obj_p=objective_given_dags_interventions(cpdag, inter_p, cpdag.copy(), full_mec, is_tree=is_tree)/ max_score
                            process_ov(inter_p, bp, k, OVs, obj_p, meth, k_range)
                        continue

                    process_ov(inter, b, k, OVs, obj, meth, k_range)

    #save plot data in json
    with open(name +'_OVs.json', 'w') as fp:
        json.dump(OVs, fp)
    with open(name + '_times.json', 'w') as fp:
        json.dump(times_dict, fp)

    #list of invalid dag tallies
    with open(name + '_invalids.json', 'w') as fp:
        json.dump(invalid_list, fp)

    return

if __name__ == '__main__':

    #first command is run id
    if len(sys.argv) > 1:
        run = int(sys.argv[1])
        np.random.seed(run)
    else:
        run = 0
        np.random.seed(42)

    #generic infinite sample experiments
    for n in [10,20,40]:

        if n > 20:
            meths = ['rand', 'ss_a', 'ss_b', 'ss_a_cont', 'ss_b_cont', 'cont', 'drg']
            labs = ['rand', 'ss_a', 'ss_b', 'ss_a_cont','ss_b_cont', 'cont', 'drg']
            k_range = [1, 2, 3, 4, 5]
        else:
            meths = ['rand', 'ss_a', 'ss_b', 'ss_a_cont', 'ss_b_cont', 'cont', 'drg']
            labs = ['rand', 'ss_a', 'ss_b', 'ss_a_cont', 'ss_b_cont', 'cont', 'drg']
            k_range = [1, 2, 3]

        run_experiment(n, 'tree', meths, labs, k_range, title ="Mean Objective Value on Tree Graph n=" + str(n), name ='figures/tree_n=' + str(n)+ '_' + str(run), repeats=1)
        print("progress")

        if n in [10,20]:
            run_experiment(n, 'kstar', meths, labs, k_range, title ="Mean Objective Value on Star Forest Graph n=" + str(n), name ='figures/star_n=' + str(n)+ '_' + str(run), repeats=1)
            print("progress")

            run_experiment(n, 'ER_0.5', meths, labs, k_range, title ="Mean Objective Value on ER (rho=0.5) Graph n=" + str(n), name ='figures/ER_0.5_n=' + str(n)+ '_' + str(run), repeats=2)
            print("progress")

        run_experiment(n, 'ER_0.25', meths, labs, k_range, title ="Mean Objective Value on ER (rho=0.25) Graph n=" + str(n), name ='figures/ER_0.25_n=' + str(n)+ '_' + str(run), repeats=2)
        print("progress")

        run_experiment(n, 'ER_0.1', meths, labs, k_range, title ="Mean Objective Value on ER (rho=0.1) Graph n=" + str(n), name ='figures/ER_0.1_n=' + str(n)+ '_' + str(run), repeats=2)
        print("progress")

    #dream experiments
    k_range = [1,2,3,4,5]
    n=50
    meths = ['rand', 'ss_a', 'ss_b', 'cont']
    labs = ['rand', 'ss_a', 'ss_b', 'cont']
    exp = (run%5) +1
    run_experiment(n, 'dream_'+str(exp), meths, labs, k_range, title ="Mean Objective Value on Dream Graph n=" + str(n), name ='figures_dream/dream_'+str(exp)+'_n=' + str(n)+ '_' + str(run), repeats=1)




